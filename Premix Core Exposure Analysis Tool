{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7pUv35f7rgH"
      },
      "source": [
        "**PDA Tool**\n",
        "\n",
        "```\n",
        "Version: 2.5  |  Last Updated: June 28, 2024\n",
        "Authors: Nitish Sarker (MIE), Kiki Chan (ChemE), University of Toronto\n",
        "*Note: This tool is still under development*\n",
        "```\n",
        "Description:\n",
        "\n",
        "> This Colab notebook implements the PDA Script, designed for batch processing of premix particles core exposure analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_CnqFWWFlL4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "try:\n",
        "  import cv2\n",
        "except ModuleNotFoundError:\n",
        "  !pip install opencv-python  # Install OpenCV if not found\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import plotly.express as px\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm_notebook\n",
        "import re\n",
        "import warnings\n",
        "import os\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ5NGCHaZkPH",
        "outputId": "cecd72e6-1f3a-4059-8fd4-b91f760bf097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6kLixm-8l03"
      },
      "outputs": [],
      "source": [
        "# Custom functions\n",
        "# Run this cell to load all the custom functions\n",
        "\n",
        "def get_filenames(dir):        # For reading files directly from Google Drive\n",
        "\n",
        "  # Arrays for filenames (file path) and image ID (truncated file name)\n",
        "  filenames = np.array([])\n",
        "  file_ids = np.array([])\n",
        "\n",
        "  for file in os.listdir(dir):    # Reads all files in directory\n",
        "    if file.endswith('.JPG'):    # Selects only files with .jpeg ending\n",
        "      filenames = np.append(filenames, os.path.join(dir, file))     # Get file path\n",
        "      file_ids = np.append(file_ids,os.path.basename(file)[:-4])    # Truncate file path down to just its name, remove .JPG extension\n",
        "\n",
        "  #print(filenames)\n",
        "  #print(file_ids)\n",
        "\n",
        "  return filenames, file_ids\n",
        "\n",
        "def resize_with_aspect_ratio(image, width = 720):\n",
        "    original_height, original_width = image.shape[:2]                             # Get the original image dimensions\n",
        "    aspect_ratio = original_width / original_height                               # Calculate the aspect ratio\n",
        "    new_height = int(width / aspect_ratio)                                        # Calculate the new height based on the desired width\n",
        "    resized_image = cv2.resize(image, (width, new_height))                        # Resize the image\n",
        "\n",
        "    return resized_image\n",
        "\n",
        "def remove_green_background(color_list, hsv_image, original_image, has_salt):\n",
        "  lower_green = np.array(color_list[0])\n",
        "  upper_green = np.array(color_list[1])\n",
        "  mask_inv = cv2.inRange(hsv_image, lower_green, upper_green)                         # Create a binary mask for green color\n",
        "  kernel = np.ones((5, 5), np.uint8)                                              # Morphological operations to clean up the mask\n",
        "  mask_inv = cv2.erode(mask_inv, kernel, iterations=1)                                    #...\n",
        "  mask_inv = cv2.dilate(mask_inv, kernel, iterations=2)                                   #...\n",
        "  mask = cv2.bitwise_not(mask_inv)                                                #...\n",
        "\n",
        "  if has_salt:\n",
        "    blank_image = np.zeros(mask.shape[:2], dtype=\"uint8\")       # black canvas for drawing contours aboves threshold area\n",
        "    threshold_area = 100              # Minimum contour area threshold for noise\n",
        "    contours, hierarachy = cv2.findContours(mask,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)     # Exxtract only the external contours\n",
        "\n",
        "    for cnt in contours:\n",
        "      area = cv2.contourArea(cnt)\n",
        "      if area > threshold_area:\n",
        "        big_contours = cv2.drawContours(blank_image, [cnt], -1, 255, -1)\n",
        "\n",
        "    mask = cv2.bitwise_and(mask, mask, mask=big_contours)\n",
        "\n",
        "\n",
        "  subtracted_image = cv2.bitwise_and(original_image, original_image, mask=mask)  # Subtraction\n",
        "\n",
        "  return subtracted_image\n",
        "\n",
        "\n",
        "def segmented_colormapping(binary_image, subtracted_image, dark_pixel = [0,0,255], bright_pixel = [0,255,0]):\n",
        "  color_image = np.zeros((binary_image.shape[0], binary_image.shape[1], 3), dtype=np.uint8)   # Create an empty color image (BGR)\n",
        "  for y in range(binary_image.shape[0]):                                          # Iterating through the binary image\n",
        "      for x in range(binary_image.shape[1]):\n",
        "          if binary_image[y, x] == 255:                                           # Bright pixel\n",
        "              color_image[y, x] = bright_pixel                                    # Green (BGR)\n",
        "          elif binary_image[y,x] == 0:                                            # Dark pixel\n",
        "            if not all(subtracted_image[y, x] == [0, 0, 0]):                      # Adjust [0, 0, 0] if needed\n",
        "                color_image[y, x] = dark_pixel                                    # Red (BGR)\n",
        "  return color_image\n",
        "\n",
        "def exposure_estimate(binary_image, gray_image):\n",
        "  total_pixel_count = np.sum(gray_image>0)\n",
        "  nonexposed_count = np.sum(binary_image == 255)\n",
        "  exposed_count = np.sum((binary_image == 0) & (gray_image>0))\n",
        "  exposed_percent = exposed_count / total_pixel_count*100\n",
        "\n",
        "  return total_pixel_count, nonexposed_count, exposed_count, exposed_percent\n",
        "\n",
        "def generate_report(filename, original, gray, binary, segmented, exposure, dir, degradation_threshold):\n",
        "  identifier = os.path.basename(filename)\n",
        "  matches = re.findall(r'initial|core|small|large|slow|fast|[\\d]+', identifier)\n",
        "  identifier_num = '_'.join(matches)\n",
        "\n",
        "  # Add scale to the original images, note that this scale is only for 720 px wide images\n",
        "  original_with_scale = cv2.line(original, (30,70),(145,70),(0,0,0),15)\n",
        "  original_with_scale = cv2.putText(original_with_scale,'0.5 mm',(25,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2)\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
        "  fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.05, hspace=0.0)\n",
        "  axes[0].imshow(cv2.cvtColor(original_with_scale, cv2.COLOR_BGR2RGB))\n",
        "  axes[0].axis('off')\n",
        "  axes[1].imshow(cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB))\n",
        "  axes[1].axis('off')\n",
        "  axes[2].imshow(cv2.cvtColor(binary, cv2.COLOR_BGR2RGB))\n",
        "  axes[2].axis('off')\n",
        "  axes[3].imshow(cv2.cvtColor(segmented, cv2.COLOR_BGR2RGB))\n",
        "  axes[3].axis('off')\n",
        "  axes[4].text(0.4, 0.4, f'{identifier_num} \\nDegradation threshold: {degradation_threshold} \\Core exposure: {exposure:.2f}%', ha='center',fontdict={'family': 'serif', 'weight': 'bold', 'size': 12})\n",
        "  axes[4].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Save output plot onto Google drive\n",
        "  output_dir = os.path.dirname(dir) + '/processedImages/'          # Saves processed images in the folder processedImages in the directory 1 level above current\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "  plt.savefig(os.path.join(output_dir,f'{identifier_num}.png'), bbox_inches='tight')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return\n",
        "\n",
        "def sensitivity_panel(file, dir, threshold_range, exposure):\n",
        "\n",
        "  identifier = os.path.basename(file)\n",
        "  matches = re.findall(r'initial|core|small|large|slow|fast|unfortified|[\\d]+', identifier)\n",
        "  identifier_num = '_'.join(matches)\n",
        "\n",
        "  # Get the original image file and resize it\n",
        "  original = cv2.imread(file)\n",
        "  original = resize_with_aspect_ratio(original, 720)\n",
        "\n",
        "  # Add scale to the original images, note that this scale is only for 720 px wide images\n",
        "  original_with_scale = cv2.line(original, (30,70),(145,70),(0,0,0),15)       # Length of scale line determined with GIMP measuring tool first.\n",
        "  original_with_scale = cv2.putText(original_with_scale,'0.5 mm',(25,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2)    # Text on scale\n",
        "\n",
        "  base_image_path = dir + '/Sensitivity analysis/'                                # Directory with the segmented images\n",
        "\n",
        "  if len(threshold_range) < 3:\n",
        "    columns = len(threshold_range) + 1    # Number of columns in the plot\n",
        "    rows = 1                             # Number of rows in the plot\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=(3*(columns+1), 2.7))\n",
        "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.05, hspace=0.0)\n",
        "\n",
        "    # Put original image in sensitivity panel\n",
        "    axes[0].imshow(cv2.cvtColor(original_with_scale, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].axis('off')\n",
        "    axes[0].title.set_text(f'Original ({identifier_num})')\n",
        "\n",
        "    for i in range(len(threshold_range)):       # If all images fit in one row. Loop through all segemented images for each threshold and place on first row\n",
        "      img = cv2.imread(base_image_path+f'{identifier_num}_{threshold_range[i]}.png')\n",
        "      axes[i+1].imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "      axes[i+1].axis('off')\n",
        "      axes[i+1].title.set_text(f'Degradation threshold: {threshold_range[i]}')\n",
        "      axes[i+1].annotate(f'Core exposure: {exposure.iloc[i]:.2f}%', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=12)\n",
        "\n",
        "  else:\n",
        "    columns = 3       # If many imaves, place segmented images according to the a 3 column x n rows arrangement\n",
        "    rows = math.ceil((len(threshold_range) + 1 ) / columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=(3*(columns), 2.7*rows))\n",
        "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.05, hspace=0.0)\n",
        "    axes[0,0].axis('off')\n",
        "    axes[0,0].imshow(cv2.cvtColor(original_with_scale, cv2.COLOR_BGR2RGB))\n",
        "    axes[0,0].title.set_text(f'Original ({identifier_num})')\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    for j in range(rows):\n",
        "      if j == 0:\n",
        "        start_col = 1\n",
        "      else:\n",
        "        start_col = 0\n",
        "      for i in range(columns - start_col):\n",
        "        if k >= len(threshold_range):\n",
        "          axes[j,i+start_col].axis('off')\n",
        "        else:\n",
        "          img = cv2.imread(base_image_path+f'{identifier_num}_{threshold_range[k]}.png')\n",
        "          axes[j,i+start_col].imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "          axes[j,i+start_col].axis('off')\n",
        "          axes[j,i+start_col].title.set_text(f'Degradation threshold: {threshold_range[k]}')\n",
        "          axes[j,i+start_col].annotate(f'Core exposure: {exposure.iloc[k]:.2f}%', xy=(0.5, -0.1), xycoords='axes fraction', ha='center', va='center', fontsize=12)\n",
        "          k = k + 1\n",
        "\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Save output plot onto Google drive in the same directory as the segmented images\n",
        "  if not os.path.exists(base_image_path):\n",
        "    os.makedirs(base_image_path)\n",
        "  plt.savefig(os.path.join(base_image_path,f'x_{identifier_num}.png'), bbox_inches='tight')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return\n",
        "\n",
        "def summary_tables(df,cols):\n",
        "\n",
        "  df.columns = cols\n",
        "\n",
        "  f = open(os.path.join(dir, 'exposure.txt'),'w')                              # Write to text file in the same folder as the raw images\n",
        "  f.write('Core exposure estimate summary table \\nPrinted on: ' + time.strftime('%d/%m/%Y %T') + '\\n\\n')\n",
        "  f.write(tabulate(df,headers='keys',tablefmt='plain'))\n",
        "  f.close()\n",
        "\n",
        "  return\n",
        "\n",
        "def batch_processing(filenames, background, degradation_threshold, dir, has_salt, report = True):\n",
        "\n",
        "  # Create arrays for summary tables\n",
        "  list_exposed_percent = np.array([])\n",
        "\n",
        "  for file in tqdm_notebook(filenames):\n",
        "    fileinput = cv2.imread(file)\n",
        "    image = resize_with_aspect_ratio(fileinput, 720)                      # Load image, resized for easy viewing and formatting. Change desired width as needed, default = 720.\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)                                  # Convert image to HSV color space\n",
        "    subtracted_image = remove_green_background(background, hsv, image, has_salt)            # Delete background\n",
        "    gray_image = cv2.cvtColor(subtracted_image, cv2.COLOR_BGR2GRAY)               # Convert image to grayscale\n",
        "    _, binary_image = cv2.threshold(gray_image, degradation_threshold, 255, cv2.THRESH_BINARY)        # Threshold the grayscale image to separate bright and dark pixels\n",
        "    segmented_image = segmented_colormapping(binary_image, subtracted_image, [0,0,255], [0,255,0])    # Create segmented image (darker/brighter pixels)\n",
        "    total_pixel_count, nonexposed_count, exposed_count, exposed_percent = exposure_estimate(binary_image, gray_image)\n",
        "\n",
        "\n",
        "    list_exposed_percent = np.append(list_exposed_percent,exposed_percent)    # Append summary table lists with each processed image\n",
        "\n",
        "    # Save output images onto Google drive\n",
        "    identifier = os.path.basename(file)\n",
        "    matches = re.findall(r'initial|core|small|large|slow|fast|unfortified|[\\d]+', identifier)\n",
        "    identifier_num = '_'.join(matches)\n",
        "    output_dir = dir + '/Sensitivity analysis/'\n",
        "    if not os.path.exists(output_dir):                  # Create the folder if it does not exist yet\n",
        "      os.makedirs(output_dir)\n",
        "    save_image = cv2.imwrite(os.path.join(output_dir,f'{identifier_num}_{degradation_threshold}.png'),segmented_image)\n",
        "\n",
        "\n",
        "    if report:\n",
        "      generate_report(file, image, gray_image, binary_image, segmented_image, exposed_percent, dir, degradation_threshold)\n",
        "\n",
        "\n",
        "\n",
        "  return list_exposed_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmENQ2vpEHxh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Select background colour list. Uncomment the appropriate one below.\n",
        "background = [[50, 50, 0],[70, 255, 255]]                    # Green, for images with only premix\n",
        "#background = [[32, 33, 0],[70, 255, 255]]                    # Green and yellow, for images with premix extracted from bouillon\n",
        "#background = [[50, 30, 0],[80, 255, 255]]                     # Green, for premix and salt images. Filters by saturation.\n",
        "\n",
        "\n",
        "# Step 2: Specify directory containing the images to be analyzed\n",
        "dir = '/content/drive/MyDrive/premixImages/Test'     # Google drive file directory\n",
        "filenames, file_ids = get_filenames(dir)\n",
        "print('Found ' + str(len(filenames)) + ' files in the directory \\n\\n')\n",
        "\n",
        "# Step 3: Turn on the appropriate flags for reports and type of analysis\n",
        "has_salt = False\n",
        "sensitivityAnalysis = True\n",
        "report = False\n",
        "\n",
        "# Step 4: Change these parameters for degradation threshold(s)\n",
        "start = 70      # start of degradation threshold to try, required\n",
        "end = 150       # end of degradation threshold to try, required for sensitivity analysis\n",
        "stepsize = 10   # stepsize of degradation threshold, required for sensitivity analysis\n",
        "\n",
        "#-------------------------------------------------------#\n",
        "\n",
        "df = pd.DataFrame(index = file_ids)\n",
        "\n",
        "if not sensitivityAnalysis:\n",
        "\n",
        "  degradation_threshold = start\n",
        "  threshold_range = [start]         # Threshold range has to be a list to accommodate sensitivity analysis flag\n",
        "  list_exposed_percent = batch_processing(filenames, background, degradation_threshold, dir, has_salt, report)\n",
        "  df[0]=list_exposed_percent\n",
        "  print(f'Completed analysis at degradation threshold {degradation_threshold}\\n')\n",
        "\n",
        "else:\n",
        "\n",
        "  num_iterations = int((end - start)/stepsize)\n",
        "  threshold_range = range(start,end,stepsize)\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    degradation_threshold = start + i * stepsize\n",
        "    list_exposed_percent = batch_processing(filenames, background, degradation_threshold, dir, has_salt, report)\n",
        "    df[i]=list_exposed_percent\n",
        "    print(f'Completed analysis for degradation threshold at {degradation_threshold}\\n')\n",
        "\n",
        "  for j in range(len(filenames)):\n",
        "    degradation = df.iloc[j]              # Getting only the degradation values from the dataframe\n",
        "    file = filenames[j]\n",
        "    sensitivity_panel(file, dir, threshold_range, degradation)\n",
        "\n",
        "\n",
        "summary_tables(df, threshold_range)     # Save summary tables for specified degradation range\n",
        "\n",
        "#print(tabulate(df,headers='keys',tablefmt='plain'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}